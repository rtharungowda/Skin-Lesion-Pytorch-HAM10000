{"cells":[{"metadata":{"_uuid":"5719830f-fe32-4284-85e2-43cb90eb92a5","_cell_guid":"b7b14981-9b57-4a03-b9d2-0a1fb8c5d4f9","trusted":true},"cell_type":"code","source":"# %matplotlib inline\n# python libraties\nimport os, cv2,itertools\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg \nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom glob import glob\nfrom PIL import Image\n# pytorch libraries\nimport torch\nfrom torch import optim,nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader,Dataset\nfrom torchvision import models,transforms\n\n# sklearn libraries\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# to make the results are reproducible\nnp.random.seed(10)\ntorch.manual_seed(10)\ntorch.cuda.manual_seed(10)\n\nprint(os.listdir(\"../input/skin-cancer-mnist-ham10000/\"))","execution_count":57,"outputs":[{"output_type":"stream","text":"['hmnist_28_28_RGB.csv', 'ham10000_images_part_1', 'HAM10000_images_part_2', 'hmnist_28_28_L.csv', 'HAM10000_images_part_1', 'HAM10000_metadata.csv', 'hmnist_8_8_RGB.csv', 'hmnist_8_8_L.csv', 'ham10000_images_part_2']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Feature eng and data preprocessing"},{"metadata":{"_uuid":"87800643-6f14-4c05-8a8b-8b67a60b584e","_cell_guid":"dde36df4-dad1-4c1b-8984-b23618db1ac5","trusted":true},"cell_type":"code","source":"data_dir = '../input/skin-cancer-mnist-ham10000'\nall_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\n#extracts the image id to match it with the .csv label file\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0] : x for x in all_image_path}\nlesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'dermatofibroma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}\ninput_size = 224","execution_count":58,"outputs":[]},{"metadata":{"_uuid":"5e7fbf4e-bb19-42e0-8916-2c8cf447023d","_cell_guid":"2a286f99-5a01-4114-8ab8-4b2a7ff93a27","trusted":true},"cell_type":"code","source":"# print(os.path.join(data_dir,'*','*.jpg'))\n# print(glob(os.path.join(data_dir,'*','*.jpg')))","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"f02b1143-2b1b-4600-9964-87fdd3e90f6c","_cell_guid":"34efdd85-91ad-46bb-9053-853bee4a4d6a","trusted":true},"cell_type":"code","source":"# {os.path.splitext(os.path.basename(x))[0] : x for x in all_image_path}","execution_count":60,"outputs":[]},{"metadata":{"_uuid":"e4a04fca-0fff-41d7-8f80-a5ddb3732fdf","_cell_guid":"ff290d6d-2128-417b-aed5-183c28acf265","trusted":true},"cell_type":"code","source":"#use this training model from scratch or not using pretrained model\ndef compute_img_mean_std(image_paths):\n    '''\n    normalising data from 0-255 to 0-1\n    mean and std of three channnel\n    '''\n    img_h, img_w = input_size,input_size\n    imgs = []\n    means, stds = [],[]\n    #resizing \n    for i in tqdm(image_paths):\n        img = cv2.imread(i)\n        img.resize(img_w,img_h)\n        imgs.append(img)\n        \n    # (224,224,3),nof_img -> 224,224,3,nof_img\n    imgs = np.stack(imgs, axis=3)\n    #normalising \n    imgs=imgs.astype(np.float32)/255.\n    \n    print(imgs.shape)\n    #mean and std of each channel\n    for i in range(3):\n        pixels = imgs[:,:,i,:].ravel() #resizing to one row\n        mean = pixels.mean()\n        std = pixels.std()\n        \n        means.append(mean)\n        stds.append(std)\n        \n    means.reverse() #bgr to rgb\n    std.reverse()\n    \n    print(f'mean {means}')\n    print(f'std {stds}')\n    return means, stds","execution_count":61,"outputs":[]},{"metadata":{"_uuid":"12de21dc-a932-45e0-9266-1aaeefb427c4","_cell_guid":"f038f823-83b6-494f-ada0-718b16d16413","trusted":true},"cell_type":"code","source":"# mean, std = compute_img_mean_std(all_image_path)","execution_count":62,"outputs":[]},{"metadata":{"_uuid":"fa3beb7d-72ee-48e6-842e-41dc01f69131","_cell_guid":"fa57fdfa-d571-4084-8f5c-7094bec66b9a","trusted":true},"cell_type":"code","source":"# pd.read_csv(os.path.join(data_dir,'HAM10000_metadata.csv'))","execution_count":63,"outputs":[]},{"metadata":{"_uuid":"5f7de207-c9c1-417b-b7ba-e6e4cdd73a4c","_cell_guid":"57c8afe9-ad6f-4e9d-8ed0-2fab1655c811","trusted":true},"cell_type":"code","source":"#creating three new columns image_path (path)\n#disease type full name from lesion_types (cell_type)\n#categorical code of label or disease type (cell_type_idx)\ndf = pd.read_csv(os.path.join(data_dir,'HAM10000_metadata.csv'))\n# .map maps value from keys\n#.get returns all values\ndf['path'] = df['image_id'].map(imageid_path_dict.get) \ndf['cell_type'] = df['dx'].map(lesion_type_dict.get)\ndf['cell_type_idx'] = pd.Categorical(df['cell_type']).codes\ndf.head()","execution_count":64,"outputs":[{"output_type":"execute_result","execution_count":64,"data":{"text/plain":"     lesion_id      image_id   dx dx_type   age   sex localization  \\\n0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n\n                                                path  \\\n0  ../input/skin-cancer-mnist-ham10000/HAM10000_i...   \n1  ../input/skin-cancer-mnist-ham10000/HAM10000_i...   \n2  ../input/skin-cancer-mnist-ham10000/HAM10000_i...   \n3  ../input/skin-cancer-mnist-ham10000/HAM10000_i...   \n4  ../input/skin-cancer-mnist-ham10000/ham10000_i...   \n\n                        cell_type  cell_type_idx  \n0  Benign keratosis-like lesions               2  \n1  Benign keratosis-like lesions               2  \n2  Benign keratosis-like lesions               2  \n3  Benign keratosis-like lesions               2  \n4  Benign keratosis-like lesions               2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>cell_type_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0027419</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/skin-cancer-mnist-ham10000/HAM10000_i...</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0025030</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/skin-cancer-mnist-ham10000/HAM10000_i...</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0026769</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/skin-cancer-mnist-ham10000/HAM10000_i...</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0025661</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/skin-cancer-mnist-ham10000/HAM10000_i...</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0001466</td>\n      <td>ISIC_0031633</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>ear</td>\n      <td>../input/skin-cancer-mnist-ham10000/ham10000_i...</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"ebe946c6-2bef-4a80-bcb7-b282bc3f228b","_cell_guid":"51620111-9c17-4ed0-91fa-516f473b1df7","trusted":true},"cell_type":"code","source":"# df.groupby('lesion_id').count()","execution_count":65,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Duplicates have same HAM number or lesion id\nHence they have more than 1 count values under groupby"},{"metadata":{"_uuid":"3a7d3f3c-51a5-424f-9824-feaed0dd7d48","_cell_guid":"3a6b20e7-e62b-4f56-97bd-5b1bca69b76c","trusted":true},"cell_type":"code","source":"#finding number of images in each group\nndf = df.groupby('lesion_id').count()\n#finding out lesion id that have only one image\nndf = ndf[ndf['image_id']==1]\nndf.head()","execution_count":66,"outputs":[{"output_type":"execute_result","execution_count":66,"data":{"text/plain":"             image_id  dx  dx_type  age  sex  localization  path  cell_type  \\\nlesion_id                                                                     \nHAM_0000001         1   1        1    1    1             1     1          1   \nHAM_0000003         1   1        1    1    1             1     1          1   \nHAM_0000004         1   1        1    1    1             1     1          1   \nHAM_0000007         1   1        1    1    1             1     1          1   \nHAM_0000008         1   1        1    1    1             1     1          1   \n\n             cell_type_idx  \nlesion_id                   \nHAM_0000001              1  \nHAM_0000003              1  \nHAM_0000004              1  \nHAM_0000007              1  \nHAM_0000008              1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>cell_type_idx</th>\n    </tr>\n    <tr>\n      <th>lesion_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>HAM_0000001</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>HAM_0000003</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>HAM_0000004</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>HAM_0000007</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>HAM_0000008</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"0dbfb6ef-d8c4-4f63-867d-d9e8f376d027","_cell_guid":"6e276861-e58a-40c1-8411-646627df4fb8","trusted":true},"cell_type":"code","source":"ndf.reset_index(inplace=True)\nndf.head()","execution_count":67,"outputs":[{"output_type":"execute_result","execution_count":67,"data":{"text/plain":"     lesion_id  image_id  dx  dx_type  age  sex  localization  path  \\\n0  HAM_0000001         1   1        1    1    1             1     1   \n1  HAM_0000003         1   1        1    1    1             1     1   \n2  HAM_0000004         1   1        1    1    1             1     1   \n3  HAM_0000007         1   1        1    1    1             1     1   \n4  HAM_0000008         1   1        1    1    1             1     1   \n\n   cell_type  cell_type_idx  \n0          1              1  \n1          1              1  \n2          1              1  \n3          1              1  \n4          1              1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>cell_type_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000001</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000003</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0000004</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0000007</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0000008</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"eaa050b9-7bd7-4aff-a08c-c3ad7f370e81","_cell_guid":"6e558863-efd4-4f00-837c-ea9d08e1b126","trusted":true},"cell_type":"code","source":"#identify ones with duplicate images and only one image\ndef get_duplicate(x):\n    uniq = list(ndf['lesion_id'])\n    if x in uniq:\n        return 'unduplicate'\n    return 'duplicated'\n\n#new column of lesion id\ndf['duplicates'] = df['lesion_id']\n#applying function to this column\ndf['duplicates'] = df['duplicates'].apply(get_duplicate)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['duplicates'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filtering images which are not duplicated\ndf_undup = df[df['duplicates']=='unduplicate']\ndf_undup.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating validation set\ny = df_undup['cell_type_idx']\n_, df_val = train_test_split(df_undup, test_size=0.20,random_state=101, stratify=y)\ndf_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_val['cell_type_idx'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating training set on df (including duplicates)\n#Function identifies if an image is part of the train or val set.\ndef get_val_rows(x):\n    val_list = list(df_val['image_id'])\n    if str(x) in val_list:\n        return 'val'\n    return 'train'\n\n#applying it\ndf['train_or_val'] = df['image_id']\ndf['train_or_val'] = df['train_or_val'].apply(get_val_rows)\n#filter out train rows\ndf_train = df[df['train_or_val']=='train']\nlen(df_train), len(df_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['cell_type_idx'].value_counts(), df_val['cell_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as seen above there is a serious imbalance in images per class"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating copies to balance\ndata_aug_rate = [15,10,5,50,0,40,5]\nfor i in range(7):\n    if data_aug_rate[i]>0:\n        df_train = df_train.append([df_train.loc[df_train['cell_type_idx']==i,:]]*(data_aug_rate[i]-1),ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['cell_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.reset_index().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=df_train.reset_index()\ndf_val=df_val.reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n# Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# transfer learning \n#learning by updating pre trained weights or not and updating last layer only\ndef set_para_req_grad(model, grad):\n    if grad == True:\n        for param in mode.parameters():\n            param.requires_grad = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.resnet50(),models.densenet121()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialise_model(model_name, num_classes, grad, use_pretrained=True):\n    '''\n    grad = if the pretrained weights be updated\n    use_pretrained = use pre trained weights or start from scratch\n    '''\n    model = None\n    \n    if model_name == 'resnet':\n        model = models.resnet50(pretrained=use_pretrained)\n        set_para_req_grad(model, grad)\n        num_ftrs = model.fc.in_features\n        model.fc = nn.Linear(num_ftrs, num_classes)\n    elif model_name == 'densenet':\n        model = models.densenet121(pretrained=use_pretrained)\n        set_para_req_grad(model,grad)\n        num_ftrs = model.classifier.in_features\n        model.classifier = nn.Linear(num_ftrs, num_classes)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 7\nmodel_name = 'densenet'\ngrad = False\n#initialise model to run\nmodel = initialise_model(model_name, num_classes, grad, use_pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training model on gpu \ndevice = torch.device('cuda:0')\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if not training from scratch and using pretrained\nmean = (0.49139968, 0.48215827, 0.44653124)\nstd = (0.24703233, 0.24348505, 0.26158768)\n#data augmentation\ntrain_transform = transforms.Compose([transforms.Resize((input_size,input_size)),\n                                     transforms.RandomHorizontalFlip(),\n                                     transforms.RandomVerticalFlip(),\n                                     transforms.RandomRotation(20),\n                                     transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize(mean, std)])\nval_transform = transforms.Compose([transforms.Resize((input_size,input_size)),\n                                   transforms.ToTensor(),\n                                   transforms.Normalize(mean,std)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataloader or dataset\nclass HAM1000(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __len__(self):\n        return(len(self.df))\n    \n    def __getitem__(self, index):\n        X = Image.open(self.df['path'][index])\n        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n        \n        if self.transform:\n            X = self.transform(X)\n        return X,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset\ntrain_ds = HAM1000(df_train, transform=train_transform)\nval_ds = HAM1000(df_val, transform=val_transform)\n\nbatch_size = 32\nepochs = 10\nlr = 1e-3\n\n#dataloader\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True,num_workers=4)\nval_dl = DataLoader(val_ds, batch_size=batch_size,shuffle=False, num_workers=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimiser and criterion\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss().to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loss and accuracy calculator\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val*n\n        self.count += n\n        self.avg = self.sum/self.count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tloss_train , tacc_train =[],[]\ndef train(train_loader, model, criterion, optimizer, epoch):\n    model.train()\n    train_loss = AverageMeter()\n    train_acc = AverageMeter()\n    curr_iter = (epoch-1)*len(train_dl)\n    \n    for i,data in enumerate(train_loader):\n        images, labels = data\n        n = images.size(0)\n        \n        images = Variable(images).to(device)\n        labels = Variable(labels).to(device)\n     \n        optimizer.zero_grad()\n        outputs = model(images)\n        \n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        pred = torch.argmax(outputs,dim=1)\n        train_acc.update(pred.eq(labels).sum().item()/n)\n        train_loss.update(loss.item())\n        curr_iter += 1\n        if (i+1)%100 == 0:\n            print(f'epoch {epoch} [iter {i+1}/{len(train_dl)}] [train loss {train_loss.avg:.5f}] [train acc {train_acc.avg:.5f}]')\n            tloss_train.append(train_loss.avg)\n            tacc_train.append(train_acc.avg)\n        \n    return train_loss.avg, train_acc.avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate(val_loader, model, criterion, optimizer, epoch):\n    model.eval()\n    val_loss = AverageMeter()\n    val_acc = AverageMeter()\n    with torch.no_grad():\n        for i, data in enumerate(val_loader):\n            images, labels = data\n            n = images.size(0)\n            images = Variable(images).to(device)\n            labels = Variable(labels).to(device)\n            \n            output = model(images)\n            pred = torch.argmax(output, dim=1)\n            \n            val_acc.update(pred.eq(labels).sum().item()/n)\n            val_loss.update(criterion(output, labels).item())\n            \n        print('-----------------------------------------')\n        print(f'[epoch {epoch}] [iter{i/len(val_dl)}] [val loss {val_loss.avg:.5f}] [val_acc {val_acc.avg:.5f}]')\n        print('-----------------------------------------')\n            \n        return val_loss.avg, val_acc.avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_val_acc = 0\ntloss_val, tacc_val = [],[]\n\nfor epoch in range(epochs):\n    loss_train, acc_train = train(train_dl, model, criterion, optimizer, epoch)\n    loss_val, acc_val = validate(val_dl, model, criterion, optimizer, epoch)\n    tloss_val.append(loss_val)\n    tacc_val.append(acc_val)\n    \n    if acc_val > best_val_acc:\n        best_val_acc = acc_val\n        print('******************')\n        print(f'best [epoch{epoch}] [loss {loss_val:.5f}] [acc {acc_val:.5f}]')\n        print('******************')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_dl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(num=2)\nfig1 = fig.add_subplot(2,1,1)\nfig2 = fig.add_subplot(2,1,2)\n\nfig1.plot(tloss_train, label='training loss')\nfig1.plot(tacc_train, label='training acc')\n\nfig2.plot(tloss_val, label='val loss')\nfig2.plot(tacc_val, label='val acc')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\ny_label = []\ny_predict = []\n\nwith torch.no_grad():\n    for i,(images,labels) in enumerate(val_dl):\n        print(images.size)\n        n = images.size(0)\n        images = Variable(images).to(device)\n        output = model(images)\n        pred = torch.argmax(output, dim=1)\n        y_label.extend(labels.cpu().numpy())\n        y_predict.extend(pred.cpu().numpy())\nplot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification report\nreport = classification_report(y_label, y_predict, target_names=plot_labels)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}